{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jduck/Desktop/SWENG894/CNN_Model/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "TensorFlow version 2.18.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from shutil import copy\n",
    "from shutil import copytree, rmtree\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import coremltools as ct \n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_extract():\n",
    "  if \"food-101\" in os.listdir():\n",
    "    print(\"Dataset already exists\")\n",
    "  else:\n",
    "    print(\"Downloading the data...\")\n",
    "    !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
    "    print(\"Dataset downloaded!\")\n",
    "    print(\"Extracting data..\")\n",
    "    !tar xzvf food-101.tar.gz\n",
    "    print(\"Extraction done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the images are labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_pie',\n",
       " 'baby_back_ribs',\n",
       " 'baklava',\n",
       " 'beef_carpaccio',\n",
       " 'beef_tartare',\n",
       " 'beet_salad',\n",
       " 'beignets',\n",
       " 'bibimbap',\n",
       " 'bread_pudding',\n",
       " 'breakfast_burrito',\n",
       " 'bruschetta',\n",
       " 'caesar_salad',\n",
       " 'cannoli',\n",
       " 'caprese_salad',\n",
       " 'carrot_cake',\n",
       " 'ceviche',\n",
       " 'cheesecake',\n",
       " 'cheese_plate',\n",
       " 'chicken_curry',\n",
       " 'chicken_quesadilla',\n",
       " 'chicken_wings',\n",
       " 'chocolate_cake',\n",
       " 'chocolate_mousse',\n",
       " 'churros',\n",
       " 'clam_chowder',\n",
       " 'club_sandwich',\n",
       " 'crab_cakes',\n",
       " 'creme_brulee',\n",
       " 'croque_madame',\n",
       " 'cup_cakes',\n",
       " 'deviled_eggs',\n",
       " 'donuts',\n",
       " 'dumplings',\n",
       " 'edamame',\n",
       " 'eggs_benedict',\n",
       " 'escargots',\n",
       " 'falafel',\n",
       " 'filet_mignon',\n",
       " 'fish_and_chips',\n",
       " 'foie_gras',\n",
       " 'french_fries',\n",
       " 'french_onion_soup',\n",
       " 'french_toast',\n",
       " 'fried_calamari',\n",
       " 'fried_rice',\n",
       " 'frozen_yogurt',\n",
       " 'garlic_bread',\n",
       " 'gnocchi',\n",
       " 'greek_salad',\n",
       " 'grilled_cheese_sandwich',\n",
       " 'grilled_salmon',\n",
       " 'guacamole',\n",
       " 'gyoza',\n",
       " 'hamburger',\n",
       " 'hot_and_sour_soup',\n",
       " 'hot_dog',\n",
       " 'huevos_rancheros',\n",
       " 'hummus',\n",
       " 'ice_cream',\n",
       " 'lasagna',\n",
       " 'lobster_bisque',\n",
       " 'lobster_roll_sandwich',\n",
       " 'macaroni_and_cheese',\n",
       " 'macarons',\n",
       " 'miso_soup',\n",
       " 'mussels',\n",
       " 'nachos',\n",
       " 'omelette',\n",
       " 'onion_rings',\n",
       " 'oysters',\n",
       " 'pad_thai',\n",
       " 'paella',\n",
       " 'pancakes',\n",
       " 'panna_cotta',\n",
       " 'peking_duck',\n",
       " 'pho',\n",
       " 'pizza',\n",
       " 'pork_chop',\n",
       " 'poutine',\n",
       " 'prime_rib',\n",
       " 'pulled_pork_sandwich',\n",
       " 'ramen',\n",
       " 'ravioli',\n",
       " 'red_velvet_cake',\n",
       " 'risotto',\n",
       " 'samosa',\n",
       " 'sashimi',\n",
       " 'scallops',\n",
       " 'seaweed_salad',\n",
       " 'shrimp_and_grits',\n",
       " 'spaghetti_bolognese',\n",
       " 'spaghetti_carbonara',\n",
       " 'spring_rolls',\n",
       " 'steak',\n",
       " 'strawberry_shortcake',\n",
       " 'sushi',\n",
       " 'tacos',\n",
       " 'takoyaki',\n",
       " 'tiramisu',\n",
       " 'tuna_tartare',\n",
       " 'waffles']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './food-101/images'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the metadata of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classes.txt',\n",
       " 'labels.txt',\n",
       " 'test.json',\n",
       " 'test.txt',\n",
       " 'train.json',\n",
       " 'train.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./food-101/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a helper function provided from kaggle in the case that smaller subsets could be used to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to split dataset into train and test folders\n",
    "def prepare_data(filepath, src,dest):\n",
    "  classes_images = defaultdict(list)\n",
    "  with open(filepath, 'r') as txt:\n",
    "      paths = [read.strip() for read in txt.readlines()]\n",
    "      for p in paths:\n",
    "        food = p.split('/')\n",
    "        classes_images[food[0]].append(food[1] + '.jpg')\n",
    "\n",
    "  for food in classes_images.keys():\n",
    "    print(\"\\nCopying images into \",food)\n",
    "    if not os.path.exists(os.path.join(dest,food)):\n",
    "      os.makedirs(os.path.join(dest,food))\n",
    "    for i in classes_images[food]:\n",
    "      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
    "  print(\"Copying Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train data...\n",
      "\n",
      "Copying images into  apple_pie\n",
      "\n",
      "Copying images into  baby_back_ribs\n",
      "\n",
      "Copying images into  baklava\n",
      "\n",
      "Copying images into  beef_carpaccio\n",
      "\n",
      "Copying images into  beef_tartare\n",
      "\n",
      "Copying images into  beet_salad\n",
      "\n",
      "Copying images into  beignets\n",
      "\n",
      "Copying images into  bibimbap\n",
      "\n",
      "Copying images into  bread_pudding\n",
      "\n",
      "Copying images into  breakfast_burrito\n",
      "\n",
      "Copying images into  bruschetta\n",
      "\n",
      "Copying images into  caesar_salad\n",
      "\n",
      "Copying images into  cannoli\n",
      "\n",
      "Copying images into  caprese_salad\n",
      "\n",
      "Copying images into  carrot_cake\n",
      "\n",
      "Copying images into  ceviche\n",
      "\n",
      "Copying images into  cheesecake\n",
      "\n",
      "Copying images into  cheese_plate\n",
      "\n",
      "Copying images into  chicken_curry\n",
      "\n",
      "Copying images into  chicken_quesadilla\n",
      "\n",
      "Copying images into  chicken_wings\n",
      "\n",
      "Copying images into  chocolate_cake\n",
      "\n",
      "Copying images into  chocolate_mousse\n",
      "\n",
      "Copying images into  churros\n",
      "\n",
      "Copying images into  clam_chowder\n",
      "\n",
      "Copying images into  club_sandwich\n",
      "\n",
      "Copying images into  crab_cakes\n",
      "\n",
      "Copying images into  creme_brulee\n",
      "\n",
      "Copying images into  croque_madame\n",
      "\n",
      "Copying images into  cup_cakes\n",
      "\n",
      "Copying images into  deviled_eggs\n",
      "\n",
      "Copying images into  donuts\n",
      "\n",
      "Copying images into  dumplings\n",
      "\n",
      "Copying images into  edamame\n",
      "\n",
      "Copying images into  eggs_benedict\n",
      "\n",
      "Copying images into  escargots\n",
      "\n",
      "Copying images into  falafel\n",
      "\n",
      "Copying images into  filet_mignon\n",
      "\n",
      "Copying images into  fish_and_chips\n",
      "\n",
      "Copying images into  foie_gras\n",
      "\n",
      "Copying images into  french_fries\n",
      "\n",
      "Copying images into  french_onion_soup\n",
      "\n",
      "Copying images into  french_toast\n",
      "\n",
      "Copying images into  fried_calamari\n",
      "\n",
      "Copying images into  fried_rice\n",
      "\n",
      "Copying images into  frozen_yogurt\n",
      "\n",
      "Copying images into  garlic_bread\n",
      "\n",
      "Copying images into  gnocchi\n",
      "\n",
      "Copying images into  greek_salad\n",
      "\n",
      "Copying images into  grilled_cheese_sandwich\n",
      "\n",
      "Copying images into  grilled_salmon\n",
      "\n",
      "Copying images into  guacamole\n",
      "\n",
      "Copying images into  gyoza\n",
      "\n",
      "Copying images into  hamburger\n",
      "\n",
      "Copying images into  hot_and_sour_soup\n",
      "\n",
      "Copying images into  hot_dog\n",
      "\n",
      "Copying images into  huevos_rancheros\n",
      "\n",
      "Copying images into  hummus\n",
      "\n",
      "Copying images into  ice_cream\n",
      "\n",
      "Copying images into  lasagna\n",
      "\n",
      "Copying images into  lobster_bisque\n",
      "\n",
      "Copying images into  lobster_roll_sandwich\n",
      "\n",
      "Copying images into  macaroni_and_cheese\n",
      "\n",
      "Copying images into  macarons\n",
      "\n",
      "Copying images into  miso_soup\n",
      "\n",
      "Copying images into  mussels\n",
      "\n",
      "Copying images into  nachos\n",
      "\n",
      "Copying images into  omelette\n",
      "\n",
      "Copying images into  onion_rings\n",
      "\n",
      "Copying images into  oysters\n",
      "\n",
      "Copying images into  pad_thai\n",
      "\n",
      "Copying images into  paella\n",
      "\n",
      "Copying images into  pancakes\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  peking_duck\n",
      "\n",
      "Copying images into  pho\n",
      "\n",
      "Copying images into  pizza\n",
      "\n",
      "Copying images into  pork_chop\n",
      "\n",
      "Copying images into  poutine\n",
      "\n",
      "Copying images into  prime_rib\n",
      "\n",
      "Copying images into  pulled_pork_sandwich\n",
      "\n",
      "Copying images into  ramen\n",
      "\n",
      "Copying images into  ravioli\n",
      "\n",
      "Copying images into  red_velvet_cake\n",
      "\n",
      "Copying images into  risotto\n",
      "\n",
      "Copying images into  samosa\n",
      "\n",
      "Copying images into  sashimi\n",
      "\n",
      "Copying images into  scallops\n",
      "\n",
      "Copying images into  seaweed_salad\n",
      "\n",
      "Copying images into  shrimp_and_grits\n",
      "\n",
      "Copying images into  spaghetti_bolognese\n",
      "\n",
      "Copying images into  spaghetti_carbonara\n",
      "\n",
      "Copying images into  spring_rolls\n",
      "\n",
      "Copying images into  steak\n",
      "\n",
      "Copying images into  strawberry_shortcake\n",
      "\n",
      "Copying images into  sushi\n",
      "\n",
      "Copying images into  tacos\n",
      "\n",
      "Copying images into  takoyaki\n",
      "\n",
      "Copying images into  tiramisu\n",
      "\n",
      "Copying images into  tuna_tartare\n",
      "\n",
      "Copying images into  waffles\n",
      "Copying Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train data...\")\n",
    "prepare_data('./food-101/meta/train.txt', './food-101/images', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data...\n",
      "\n",
      "Copying images into  apple_pie\n",
      "\n",
      "Copying images into  baby_back_ribs\n",
      "\n",
      "Copying images into  baklava\n",
      "\n",
      "Copying images into  beef_carpaccio\n",
      "\n",
      "Copying images into  beef_tartare\n",
      "\n",
      "Copying images into  beet_salad\n",
      "\n",
      "Copying images into  beignets\n",
      "\n",
      "Copying images into  bibimbap\n",
      "\n",
      "Copying images into  bread_pudding\n",
      "\n",
      "Copying images into  breakfast_burrito\n",
      "\n",
      "Copying images into  bruschetta\n",
      "\n",
      "Copying images into  caesar_salad\n",
      "\n",
      "Copying images into  cannoli\n",
      "\n",
      "Copying images into  caprese_salad\n",
      "\n",
      "Copying images into  carrot_cake\n",
      "\n",
      "Copying images into  ceviche\n",
      "\n",
      "Copying images into  cheesecake\n",
      "\n",
      "Copying images into  cheese_plate\n",
      "\n",
      "Copying images into  chicken_curry\n",
      "\n",
      "Copying images into  chicken_quesadilla\n",
      "\n",
      "Copying images into  chicken_wings\n",
      "\n",
      "Copying images into  chocolate_cake\n",
      "\n",
      "Copying images into  chocolate_mousse\n",
      "\n",
      "Copying images into  churros\n",
      "\n",
      "Copying images into  clam_chowder\n",
      "\n",
      "Copying images into  club_sandwich\n",
      "\n",
      "Copying images into  crab_cakes\n",
      "\n",
      "Copying images into  creme_brulee\n",
      "\n",
      "Copying images into  croque_madame\n",
      "\n",
      "Copying images into  cup_cakes\n",
      "\n",
      "Copying images into  deviled_eggs\n",
      "\n",
      "Copying images into  donuts\n",
      "\n",
      "Copying images into  dumplings\n",
      "\n",
      "Copying images into  edamame\n",
      "\n",
      "Copying images into  eggs_benedict\n",
      "\n",
      "Copying images into  escargots\n",
      "\n",
      "Copying images into  falafel\n",
      "\n",
      "Copying images into  filet_mignon\n",
      "\n",
      "Copying images into  fish_and_chips\n",
      "\n",
      "Copying images into  foie_gras\n",
      "\n",
      "Copying images into  french_fries\n",
      "\n",
      "Copying images into  french_onion_soup\n",
      "\n",
      "Copying images into  french_toast\n",
      "\n",
      "Copying images into  fried_calamari\n",
      "\n",
      "Copying images into  fried_rice\n",
      "\n",
      "Copying images into  frozen_yogurt\n",
      "\n",
      "Copying images into  garlic_bread\n",
      "\n",
      "Copying images into  gnocchi\n",
      "\n",
      "Copying images into  greek_salad\n",
      "\n",
      "Copying images into  grilled_cheese_sandwich\n",
      "\n",
      "Copying images into  grilled_salmon\n",
      "\n",
      "Copying images into  guacamole\n",
      "\n",
      "Copying images into  gyoza\n",
      "\n",
      "Copying images into  hamburger\n",
      "\n",
      "Copying images into  hot_and_sour_soup\n",
      "\n",
      "Copying images into  hot_dog\n",
      "\n",
      "Copying images into  huevos_rancheros\n",
      "\n",
      "Copying images into  hummus\n",
      "\n",
      "Copying images into  ice_cream\n",
      "\n",
      "Copying images into  lasagna\n",
      "\n",
      "Copying images into  lobster_bisque\n",
      "\n",
      "Copying images into  lobster_roll_sandwich\n",
      "\n",
      "Copying images into  macaroni_and_cheese\n",
      "\n",
      "Copying images into  macarons\n",
      "\n",
      "Copying images into  miso_soup\n",
      "\n",
      "Copying images into  mussels\n",
      "\n",
      "Copying images into  nachos\n",
      "\n",
      "Copying images into  omelette\n",
      "\n",
      "Copying images into  onion_rings\n",
      "\n",
      "Copying images into  oysters\n",
      "\n",
      "Copying images into  pad_thai\n",
      "\n",
      "Copying images into  paella\n",
      "\n",
      "Copying images into  pancakes\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  peking_duck\n",
      "\n",
      "Copying images into  pho\n",
      "\n",
      "Copying images into  pizza\n",
      "\n",
      "Copying images into  pork_chop\n",
      "\n",
      "Copying images into  poutine\n",
      "\n",
      "Copying images into  prime_rib\n",
      "\n",
      "Copying images into  pulled_pork_sandwich\n",
      "\n",
      "Copying images into  ramen\n",
      "\n",
      "Copying images into  ravioli\n",
      "\n",
      "Copying images into  red_velvet_cake\n",
      "\n",
      "Copying images into  risotto\n",
      "\n",
      "Copying images into  samosa\n",
      "\n",
      "Copying images into  sashimi\n",
      "\n",
      "Copying images into  scallops\n",
      "\n",
      "Copying images into  seaweed_salad\n",
      "\n",
      "Copying images into  shrimp_and_grits\n",
      "\n",
      "Copying images into  spaghetti_bolognese\n",
      "\n",
      "Copying images into  spaghetti_carbonara\n",
      "\n",
      "Copying images into  spring_rolls\n",
      "\n",
      "Copying images into  steak\n",
      "\n",
      "Copying images into  strawberry_shortcake\n",
      "\n",
      "Copying images into  sushi\n",
      "\n",
      "Copying images into  tacos\n",
      "\n",
      "Copying images into  takoyaki\n",
      "\n",
      "Copying images into  tiramisu\n",
      "\n",
      "Copying images into  tuna_tartare\n",
      "\n",
      "Copying images into  waffles\n",
      "Copying Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating test data...\")\n",
    "prepare_data('./food-101/meta/test.txt', './food-101/images', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper method to create train_mini and test_mini data samples\n",
    "def dataset_mini(food_list, src, dest):\n",
    "  if os.path.exists(dest):\n",
    "    rmtree(dest) # removing dataset_mini(if it already exists) folders so that we will have only the classes that we want\n",
    "  os.makedirs(dest)\n",
    "  for food_item in food_list :\n",
    "    print(\"Copying images into\",food_item)\n",
    "    copytree(os.path.join(src,food_item), os.path.join(dest,food_item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking food items and generating separate data folders for the same\n",
    "# food_list = ['apple_pie','chicken_quesadilla','chicken_wings', 'chocolate_cake', 'cup_cakes', 'donuts', 'french_fries', 'hamburger', 'hot_dog','ice_cream', 'macaroni_and_cheese', 'pancakes', 'pizza', 'pork_chop', 'ramen', 'red_velvet_cake', 'steak', 'tacos', 'waffles', 'omelette']\n",
    "# food_list = ['apple_pie', 'chocolate_cake', 'cup_cakes', 'donuts', 'french_fries', 'hamburger', 'hot_dog','ice_cream']\n",
    "food_list = ['apple_pie', 'chocolate_cake', 'cup_cakes']\n",
    "dest_train = 'train_mini'\n",
    "src_train = 'train'\n",
    "src_test = 'test'\n",
    "dest_test = 'test_mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train data folder with new classes\n",
      "Copying images into apple_pie\n",
      "Copying images into chocolate_cake\n",
      "Copying images into cup_cakes\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train data folder with new classes\")\n",
    "dataset_mini(food_list, src_train, dest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data folder with new classes\n",
      "Copying images into apple_pie\n",
      "Copying images into chocolate_cake\n",
      "Copying images into cup_cakes\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating test data folder with new classes\")\n",
    "dataset_mini(food_list, src_test, dest_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the models features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. (Does not error when running, however with the GPU currently not being detected, hold off on the first training, as it will take roughly 1:30 hour for each epoch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multiclass_vgg16_model(input_shape, num_classes):\n",
    "    vgg = VGG16(input_shape =input_shape, weights='imagenet', include_top=False)\n",
    "    x = vgg.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128,activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    predictions = Dense(num_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=vgg.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multiclass_inception_model(num_classes):\n",
    "    inception = InceptionV3(weights='imagenet', include_top=False)\n",
    "    x = inception.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128,activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    predictions = Dense(num_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inception.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_mini'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m      9\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m     10\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m,\n\u001b[1;32m     11\u001b[0m     shear_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.175\u001b[39m,\n\u001b[1;32m     12\u001b[0m     zoom_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.175\u001b[39m,\n\u001b[1;32m     13\u001b[0m     horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m test_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m test_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     24\u001b[0m     validation_data_dir,\n\u001b[1;32m     25\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(img_height, img_width),\n\u001b[1;32m     26\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     27\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/SWENG894/CNN_Model/venv/lib/python3.9/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1122\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SWENG894/CNN_Model/venv/lib/python3.9/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    452\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    455\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_mini'"
     ]
    }
   ],
   "source": [
    "n_classes = 3\n",
    "img_width, img_height = 299, 299\n",
    "train_data_dir = 'train_mini'\n",
    "validation_data_dir = 'test_mini'\n",
    "nb_train_samples = 2250\n",
    "nb_validation_samples = 750\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.175,\n",
    "    zoom_range=0.175,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.9914 - accuracy: 0.5457\n",
      "Epoch 1: val_loss improved from inf to 0.72842, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 58s 330ms/step - loss: 0.9914 - accuracy: 0.5457 - val_loss: 0.7284 - val_accuracy: 0.7717\n",
      "Epoch 2/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.7753\n",
      "Epoch 2: val_loss improved from 0.72842 to 0.52194, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 214ms/step - loss: 0.6885 - accuracy: 0.7753 - val_loss: 0.5219 - val_accuracy: 0.8573\n",
      "Epoch 3/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.8192\n",
      "Epoch 3: val_loss improved from 0.52194 to 0.41303, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 216ms/step - loss: 0.5408 - accuracy: 0.8192 - val_loss: 0.4130 - val_accuracy: 0.8804\n",
      "Epoch 4/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4678 - accuracy: 0.8406\n",
      "Epoch 4: val_loss improved from 0.41303 to 0.34515, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 215ms/step - loss: 0.4678 - accuracy: 0.8406 - val_loss: 0.3451 - val_accuracy: 0.9035\n",
      "Epoch 5/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4067 - accuracy: 0.8684\n",
      "Epoch 5: val_loss improved from 0.34515 to 0.30762, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 217ms/step - loss: 0.4067 - accuracy: 0.8684 - val_loss: 0.3076 - val_accuracy: 0.9130\n",
      "Epoch 6/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3589 - accuracy: 0.8800\n",
      "Epoch 6: val_loss improved from 0.30762 to 0.28043, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 216ms/step - loss: 0.3589 - accuracy: 0.8800 - val_loss: 0.2804 - val_accuracy: 0.9212\n",
      "Epoch 7/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3331 - accuracy: 0.8903\n",
      "Epoch 7: val_loss improved from 0.28043 to 0.25972, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 217ms/step - loss: 0.3331 - accuracy: 0.8903 - val_loss: 0.2597 - val_accuracy: 0.9239\n",
      "Epoch 8/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.9002\n",
      "Epoch 8: val_loss improved from 0.25972 to 0.24012, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.3108 - accuracy: 0.9002 - val_loss: 0.2401 - val_accuracy: 0.9321\n",
      "Epoch 9/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.9011\n",
      "Epoch 9: val_loss improved from 0.24012 to 0.23473, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 217ms/step - loss: 0.3061 - accuracy: 0.9011 - val_loss: 0.2347 - val_accuracy: 0.9321\n",
      "Epoch 10/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.9181\n",
      "Epoch 10: val_loss improved from 0.23473 to 0.22351, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 214ms/step - loss: 0.2715 - accuracy: 0.9181 - val_loss: 0.2235 - val_accuracy: 0.9321\n",
      "Epoch 11/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9145\n",
      "Epoch 11: val_loss improved from 0.22351 to 0.21610, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 216ms/step - loss: 0.2640 - accuracy: 0.9145 - val_loss: 0.2161 - val_accuracy: 0.9375\n",
      "Epoch 12/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.9311\n",
      "Epoch 12: val_loss improved from 0.21610 to 0.21141, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 214ms/step - loss: 0.2341 - accuracy: 0.9311 - val_loss: 0.2114 - val_accuracy: 0.9470\n",
      "Epoch 13/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9373\n",
      "Epoch 13: val_loss improved from 0.21141 to 0.20610, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 219ms/step - loss: 0.2171 - accuracy: 0.9373 - val_loss: 0.2061 - val_accuracy: 0.9361\n",
      "Epoch 14/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9355\n",
      "Epoch 14: val_loss improved from 0.20610 to 0.20171, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.2221 - accuracy: 0.9355 - val_loss: 0.2017 - val_accuracy: 0.9484\n",
      "Epoch 15/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9440\n",
      "Epoch 15: val_loss improved from 0.20171 to 0.19595, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 219ms/step - loss: 0.1930 - accuracy: 0.9440 - val_loss: 0.1960 - val_accuracy: 0.9484\n",
      "Epoch 16/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1881 - accuracy: 0.9458\n",
      "Epoch 16: val_loss improved from 0.19595 to 0.19166, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.1881 - accuracy: 0.9458 - val_loss: 0.1917 - val_accuracy: 0.9511\n",
      "Epoch 17/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9575\n",
      "Epoch 17: val_loss improved from 0.19166 to 0.18390, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 217ms/step - loss: 0.1642 - accuracy: 0.9575 - val_loss: 0.1839 - val_accuracy: 0.9538\n",
      "Epoch 18/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9485\n",
      "Epoch 18: val_loss did not improve from 0.18390\n",
      "140/140 [==============================] - 30s 213ms/step - loss: 0.1776 - accuracy: 0.9485 - val_loss: 0.1866 - val_accuracy: 0.9552\n",
      "Epoch 19/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9534\n",
      "Epoch 19: val_loss improved from 0.18390 to 0.18291, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 216ms/step - loss: 0.1599 - accuracy: 0.9534 - val_loss: 0.1829 - val_accuracy: 0.9511\n",
      "Epoch 20/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9570\n",
      "Epoch 20: val_loss improved from 0.18291 to 0.18175, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.1613 - accuracy: 0.9570 - val_loss: 0.1818 - val_accuracy: 0.9524\n",
      "Epoch 21/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9651\n",
      "Epoch 21: val_loss improved from 0.18175 to 0.17812, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.1451 - accuracy: 0.9651 - val_loss: 0.1781 - val_accuracy: 0.9552\n",
      "Epoch 22/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9722\n",
      "Epoch 22: val_loss did not improve from 0.17812\n",
      "140/140 [==============================] - 30s 213ms/step - loss: 0.1292 - accuracy: 0.9722 - val_loss: 0.1793 - val_accuracy: 0.9497\n",
      "Epoch 23/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9745\n",
      "Epoch 23: val_loss improved from 0.17812 to 0.17691, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 219ms/step - loss: 0.1250 - accuracy: 0.9745 - val_loss: 0.1769 - val_accuracy: 0.9511\n",
      "Epoch 24/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9691\n",
      "Epoch 24: val_loss did not improve from 0.17691\n",
      "140/140 [==============================] - 30s 215ms/step - loss: 0.1218 - accuracy: 0.9691 - val_loss: 0.1781 - val_accuracy: 0.9524\n",
      "Epoch 25/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9696\n",
      "Epoch 25: val_loss improved from 0.17691 to 0.17210, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 217ms/step - loss: 0.1324 - accuracy: 0.9696 - val_loss: 0.1721 - val_accuracy: 0.9470\n",
      "Epoch 26/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9696\n",
      "Epoch 26: val_loss improved from 0.17210 to 0.17148, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.1274 - accuracy: 0.9696 - val_loss: 0.1715 - val_accuracy: 0.9497\n",
      "Epoch 27/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9736\n",
      "Epoch 27: val_loss did not improve from 0.17148\n",
      "140/140 [==============================] - 30s 213ms/step - loss: 0.1160 - accuracy: 0.9736 - val_loss: 0.1741 - val_accuracy: 0.9524\n",
      "Epoch 28/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9821\n",
      "Epoch 28: val_loss improved from 0.17148 to 0.16477, saving model to best_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 217ms/step - loss: 0.1073 - accuracy: 0.9821 - val_loss: 0.1648 - val_accuracy: 0.9511\n",
      "Epoch 29/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9767\n",
      "Epoch 29: val_loss did not improve from 0.16477\n",
      "140/140 [==============================] - 30s 215ms/step - loss: 0.1020 - accuracy: 0.9767 - val_loss: 0.1765 - val_accuracy: 0.9511\n",
      "Epoch 30/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9754\n",
      "Epoch 30: val_loss did not improve from 0.16477\n",
      "140/140 [==============================] - 30s 214ms/step - loss: 0.1042 - accuracy: 0.9754 - val_loss: 0.1673 - val_accuracy: 0.9538\n"
     ]
    }
   ],
   "source": [
    "model = make_multiclass_inception_model(3)\n",
    "model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('history_3class.log')\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch = nb_train_samples // batch_size,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 1.0667 - accuracy: 0.4463\n",
      "Epoch 1: val_loss improved from inf to 0.80883, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 51s 271ms/step - loss: 1.0667 - accuracy: 0.4463 - val_loss: 0.8088 - val_accuracy: 0.7092\n",
      "Epoch 2/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.7508 - accuracy: 0.6876\n",
      "Epoch 2: val_loss improved from 0.80883 to 0.53678, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 217ms/step - loss: 0.7508 - accuracy: 0.6876 - val_loss: 0.5368 - val_accuracy: 0.8220\n",
      "Epoch 3/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.7731\n",
      "Epoch 3: val_loss improved from 0.53678 to 0.41782, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.5975 - accuracy: 0.7731 - val_loss: 0.4178 - val_accuracy: 0.8723\n",
      "Epoch 4/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.7963\n",
      "Epoch 4: val_loss did not improve from 0.41782\n",
      "140/140 [==============================] - 31s 217ms/step - loss: 0.5588 - accuracy: 0.7963 - val_loss: 0.4412 - val_accuracy: 0.8370\n",
      "Epoch 5/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.8299\n",
      "Epoch 5: val_loss did not improve from 0.41782\n",
      "140/140 [==============================] - 31s 217ms/step - loss: 0.4883 - accuracy: 0.8299 - val_loss: 0.4238 - val_accuracy: 0.8519\n",
      "Epoch 6/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.8330\n",
      "Epoch 6: val_loss improved from 0.41782 to 0.38264, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.4568 - accuracy: 0.8330 - val_loss: 0.3826 - val_accuracy: 0.8791\n",
      "Epoch 7/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4272 - accuracy: 0.8536\n",
      "Epoch 7: val_loss improved from 0.38264 to 0.31198, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 219ms/step - loss: 0.4272 - accuracy: 0.8536 - val_loss: 0.3120 - val_accuracy: 0.9117\n",
      "Epoch 8/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.8577\n",
      "Epoch 8: val_loss improved from 0.31198 to 0.29836, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 220ms/step - loss: 0.3985 - accuracy: 0.8577 - val_loss: 0.2984 - val_accuracy: 0.9158\n",
      "Epoch 9/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.8759\n",
      "Epoch 9: val_loss did not improve from 0.29836\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.3629 - accuracy: 0.8759 - val_loss: 0.3188 - val_accuracy: 0.8967\n",
      "Epoch 10/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3766 - accuracy: 0.8711\n",
      "Epoch 10: val_loss improved from 0.29836 to 0.28673, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 220ms/step - loss: 0.3766 - accuracy: 0.8711 - val_loss: 0.2867 - val_accuracy: 0.9144\n",
      "Epoch 11/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3337 - accuracy: 0.8863\n",
      "Epoch 11: val_loss improved from 0.28673 to 0.27101, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 221ms/step - loss: 0.3337 - accuracy: 0.8863 - val_loss: 0.2710 - val_accuracy: 0.9198\n",
      "Epoch 12/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8944\n",
      "Epoch 12: val_loss did not improve from 0.27101\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.3175 - accuracy: 0.8944 - val_loss: 0.3016 - val_accuracy: 0.9008\n",
      "Epoch 13/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8863\n",
      "Epoch 13: val_loss did not improve from 0.27101\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.3191 - accuracy: 0.8863 - val_loss: 0.3339 - val_accuracy: 0.8927\n",
      "Epoch 14/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.9006\n",
      "Epoch 14: val_loss did not improve from 0.27101\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.3080 - accuracy: 0.9006 - val_loss: 0.2896 - val_accuracy: 0.9144\n",
      "Epoch 15/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9038\n",
      "Epoch 15: val_loss did not improve from 0.27101\n",
      "140/140 [==============================] - 31s 218ms/step - loss: 0.2839 - accuracy: 0.9038 - val_loss: 0.2755 - val_accuracy: 0.9158\n",
      "Epoch 16/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9141\n",
      "Epoch 16: val_loss improved from 0.27101 to 0.25276, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 31s 220ms/step - loss: 0.2720 - accuracy: 0.9141 - val_loss: 0.2528 - val_accuracy: 0.9239\n",
      "Epoch 17/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9230\n",
      "Epoch 17: val_loss improved from 0.25276 to 0.25057, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 216ms/step - loss: 0.2430 - accuracy: 0.9230 - val_loss: 0.2506 - val_accuracy: 0.9334\n",
      "Epoch 18/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.9176\n",
      "Epoch 18: val_loss did not improve from 0.25057\n",
      "140/140 [==============================] - 30s 213ms/step - loss: 0.2515 - accuracy: 0.9176 - val_loss: 0.2558 - val_accuracy: 0.9253\n",
      "Epoch 19/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.9221\n",
      "Epoch 19: val_loss did not improve from 0.25057\n",
      "140/140 [==============================] - 30s 216ms/step - loss: 0.2309 - accuracy: 0.9221 - val_loss: 0.2657 - val_accuracy: 0.9185\n",
      "Epoch 20/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9257\n",
      "Epoch 20: val_loss did not improve from 0.25057\n",
      "140/140 [==============================] - 30s 215ms/step - loss: 0.2303 - accuracy: 0.9257 - val_loss: 0.2547 - val_accuracy: 0.9321\n",
      "Epoch 21/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.9329\n",
      "Epoch 21: val_loss improved from 0.25057 to 0.22840, saving model to best_vgg16_model_3class.hdf5\n",
      "140/140 [==============================] - 30s 216ms/step - loss: 0.2127 - accuracy: 0.9329 - val_loss: 0.2284 - val_accuracy: 0.9361\n",
      "Epoch 22/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9418\n",
      "Epoch 22: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 214ms/step - loss: 0.1920 - accuracy: 0.9418 - val_loss: 0.2436 - val_accuracy: 0.9402\n",
      "Epoch 23/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9400\n",
      "Epoch 23: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 215ms/step - loss: 0.2030 - accuracy: 0.9400 - val_loss: 0.2511 - val_accuracy: 0.9307\n",
      "Epoch 24/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9432\n",
      "Epoch 24: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 215ms/step - loss: 0.1885 - accuracy: 0.9432 - val_loss: 0.2925 - val_accuracy: 0.8995\n",
      "Epoch 25/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9508\n",
      "Epoch 25: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 214ms/step - loss: 0.1667 - accuracy: 0.9508 - val_loss: 0.2602 - val_accuracy: 0.9361\n",
      "Epoch 26/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9440\n",
      "Epoch 26: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 214ms/step - loss: 0.1729 - accuracy: 0.9440 - val_loss: 0.4699 - val_accuracy: 0.8764\n",
      "Epoch 27/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9499\n",
      "Epoch 27: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 216ms/step - loss: 0.1713 - accuracy: 0.9499 - val_loss: 0.2376 - val_accuracy: 0.9375\n",
      "Epoch 28/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9552\n",
      "Epoch 28: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 215ms/step - loss: 0.1508 - accuracy: 0.9552 - val_loss: 0.2654 - val_accuracy: 0.9307\n",
      "Epoch 29/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9530\n",
      "Epoch 29: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 214ms/step - loss: 0.1476 - accuracy: 0.9530 - val_loss: 0.2704 - val_accuracy: 0.9293\n",
      "Epoch 30/30\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9548\n",
      "Epoch 30: val_loss did not improve from 0.22840\n",
      "140/140 [==============================] - 30s 215ms/step - loss: 0.1521 - accuracy: 0.9548 - val_loss: 0.2567 - val_accuracy: 0.9389\n"
     ]
    }
   ],
   "source": [
    "model = make_multiclass_vgg16_model(3)\n",
    "model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='best_vgg16_model_3class.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('history_3class_vgg16.log')\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch = nb_train_samples // batch_size,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=vgg16_3class_v2.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvgg16_3class_v2.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SWENG894/CNN_Model/venv/lib/python3.9/site-packages/keras/src/saving/saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: filepath=vgg16_3class_v2.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732049876.693131 6074388 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1732049876.693183 6074388 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1732049877.017430 6074388 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1732049877.017484 6074388 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1732049877.406324 6074388 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1732049877.406372 6074388 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1732049877.681953 6074388 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1732049877.682018 6074388 single_machine.cc:361] Starting new session\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 39.89 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 84/84 [00:00<00:00, 3925.19 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 610.80 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 88/88 [00:00<00:00, 160.59 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 464.92 passes/s]\n",
      "/Users/jduck/Desktop/SWENG894/CNN_Model/venv/lib/python3.9/site-packages/coremltools/converters/mil/backend/mil/load.py:838: UserWarning: Some dimensions in the input shape are unknown, hence they are set to flexible ranges with lower bound and default value = 1, and upper bound = 2. To set different values for the default shape and upper bound, please use the ct.RangeDim() method as described here: https://coremltools.readme.io/docs/flexible-inputs#set-the-range-for-each-dimension.\n",
      "  warnings.warn(\n",
      "/Users/jduck/Desktop/SWENG894/CNN_Model/venv/lib/python3.9/site-packages/coremltools/converters/mil/backend/mil/load.py:848: UserWarning: There is \"None\" dim in TF input placeholder. Please consider specifying input shapes by using the \"inputs\" param in ct.convert().\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ml_model = ct.convert(\"vgg16_model\", convert_to=\"mlprogram\",inputs=[ct.ImageType(bias=[-1,-1,-1], scale=1/255)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.save(\"vgg16_3class.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor shape=(None, None, None, 3), dtype=float32, sparse=False, name=input_layer_2>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
